# OpenAIæ¥å£è§„èŒƒ
é€šè¿‡å¯¹ä¸åŒå¤§æ¨¡å‹è¿›è¡Œå°è£…ï¼Œä»¥æä¾›ç»Ÿä¸€è¾“å…¥è¾“å‡ºçš„APIä¾›å¤–éƒ¨è°ƒç”¨ã€‚
ä»¥[chatglm(2)-6b](#https://huggingface.co/THUDM/chatglm2-6b)ä¸ºä¾‹åˆ†ä¸¤éƒ¨åˆ†æ¥ä»‹ç»[æ¥å£å¦‚ä½•è°ƒç”¨](#APIè°ƒç”¨ç¤ºä¾‹)ä»¥åŠ[å¦‚ä½•å°è£…ä»£ç ](#APIå°è£…ç¤ºä¾‹)ã€‚
## APIè°ƒç”¨ç¤ºä¾‹
### è¾“å…¥
python ç¤ºä¾‹
```
import openai
if __name__ == "__main__":
    openai.api_base = "http://localhost:8000/v1"
    openai.api_key = "none"
    for chunk in openai.ChatCompletion.create(
        model="chatglm2-6b",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©ç†"},
            {"role": "user", "content": "ä½ å¥½"}
        ],
        stream=True
    ):
        if hasattr(chunk.choices[0].delta, "content"):
            print(chunk.choices[0].delta.content, end="", flush=True)
```
requests ç¤ºä¾‹
```
import json
import requests
if __name__ == "__main__":
    header = {"Content-Type": "application/json"}
    data = {
        "model": "chatglm2-6b",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©ç†"},
            {"role": "user", "content": "ä½ å¥½"}
        ],
        "stream": True,
    }
    url = "http://localhost:8000/v1/chat/completions"
    response = requests.post(url, headers=header, data=json.dumps(data))

    for line in response.iter_lines():
        line_str = str(line, encoding='utf-8')
        if line_str.startswith("data:"):
            if line_str.startswith("data: [DONE]"):
                break
            line_json = json.loads(line_str[5:].strip())
            delta = line_json['choices'][0]['delta']
            if 'content' in delta:
                print(delta["content"], end="", flush=True)

```
Requestå‚æ•°è¯´æ˜
| å‚æ•°              | ç±»å‹      | é»˜è®¤å€¼   | è¯´æ˜                        |
| ----------------- | --------- | -------- | --------------------------- |
| model             | string    | null     | è¦ä½¿ç”¨çš„æ¨¡å‹ID              |
| messages          | array     | null     | è‡³ä»Šä¸ºæ­¢å¯¹è¯çš„æ¶ˆæ¯åˆ—è¡¨      |
| temperature       | number    | 0.95     | æ ·æœ¬æ¸©åº¦, å–å€¼0åˆ°2          |
| top_p             | number    | 0.7      | æ ¸é‡‡æ ·,å–å€¼0åˆ°1             |
| stream            | boolean   | false    | æ”¯æŒæµå¼è¿”å›                |
| max_tokens        | integer   | 2048     | èŠå¤©ä¸­ç”Ÿæˆçš„æœ€å¤§tokens      |

### è¾“å‡º
è¾“å‡ºç¤ºä¾‹ (stream=False)
```
{
  "model": "chatglm2-6b",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©ç† ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"
      },
      "finish_reason": "stop"
    }
  ],
  "created": 1689579393
}
```

## APIå°è£…ç¤ºä¾‹
python ç¤ºä¾‹
å‚è€ƒ[chatglm_api.py](https://github.com/StarSkk/openai_api/blob/main/chatglm_api.py)
